{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BKuRv-fvDlqX"
      },
      "outputs": [],
      "source": [
        "# 우리가 쓸 재료를 준비해줍니다.\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 코드를 짜기 전에 앞서, 우리가 써야 하는 gpu로 설정을 해줄게요.\n",
        "# 우리는 런타임 유형을 통해서 이미 gpu를 쓰겠다고 말해둔 상태지만,\n",
        "# 여러분이 나중에 서버에서 모델을 돌리거나, 기타 상황 등에서 직접 gpu 설정을 해줘야 할 수도 있습니다.\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu' # 이 코드는 cuda, 즉 gpu를 쓸 수 있는 환경이면 cuda를 쓰고, 아니면 cpu를 써라 라는 코드입니다.\n",
        "# 만약 cpu로 돌리면 batch 단위로 데이터가 들어가지 못하게 될 거에요.\n",
        "\n",
        "# 다음은 seed를 고정해줄텐데요,\n",
        "# seed를 고정한다는 말은 여러분과 제 결과를 동일하게 보여준다~ 는 의미입니다.\n",
        "# random seed를 고정한다는 말이 좀 더 정확한 말입니다.\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "metadata": {
        "id": "bEb-QuXfD5Aa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 그럼 이제 간단한 데이터를 넣어볼게요.\n",
        "# xor 문제인데요, 지금 들어가는 값들은 좌표 평면의 값이라고 생각하시면 됩니다.\n",
        "\n",
        "X = torch.FloatTensor([[0,0], [0,1], [1,0], [1,10]]).to(device) #0,0, 0,1, 1,0, 1,1을 tensor 형태로 입력해줄게요.\n",
        "Y = torch.FloatTensor([[0], [1], [1], [0]]).cuda() #그에 대한 label, 즉 실제값도 입력해줍니다."
      ],
      "metadata": {
        "id": "dd2ZkhzwD-D5"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 우리는 총 4개의 layer를 쌓을 건데요,\n",
        "# 첫 번째는 입력으로 들어온 2개의 값을 10개의 output으로 보내고,\n",
        "# 두 번째는 그 10개의 값을 받아서 또 10개로 내뱉고,\n",
        "# 세 번째 역시 10,10,\n",
        "# 마지막은 10개의 인풋을 하나의 output으로 바꿔주는 layer입니다.\n",
        "# 그리고 이 1개의 값이 sigmoid라는 활성화 함수를 통과해서 최종 Output이 결정되는 것입니다.\n",
        "\n",
        "# layer를 쌓는 과정은 이와 같이 layer를 먼저 결정해주고\n",
        "linear1 = torch.nn.Linear(2, 10, bias=True)\n",
        "linear2 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear3 = torch.nn.Linear(10, 10, bias=True)\n",
        "linear4 = torch.nn.Linear(10, 1, bias=True)\n",
        "sigmoid = torch.nn.Sigmoid()"
      ],
      "metadata": {
        "id": "CyPtH6yjEPQ7"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sequential을 이용해서 차곡차곡 쌓아주는 것입니다.\n",
        "# 뒤에 심화 과정에서 배우겠지만, 대부분 모델을 쌓을 때 저렇게 먼저 재료를 준비해주고 이 재료를 sequential을 이용해 하나의 블럭을 만들어주게 됩니다.\n",
        "model = torch.nn.Sequential(linear1, sigmoid, linear2, sigmoid, linear3, sigmoid, linear4, sigmoid).to(device)"
      ],
      "metadata": {
        "id": "G2r03YHpE0zL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 다음으로는 손실함수와 최적화 함수를 정의해줄게요.\n",
        "# 여기서는 BCE, Binary Cross Entropy라는 손실함수와\n",
        "# 우리가 배웠던 SGD를 이용해줄게요.\n",
        "# Binary Cross Entropy는\n",
        "criterion = torch.nn.BCELoss().to(device)\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=10)  # 처음에는 1로 하고 0.1, 10으로 차례대로 바꾸기"
      ],
      "metadata": {
        "id": "YrgqglAUE4PQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 자 이제 모델을 돌려보도록 하겠습니다.\n",
        "# 우리는 총 10001번의 epoch을 돌릴 겁니다. 즉 모델을 10001번 학습시킨다는 이야기죠.\n",
        "for step in range(10001):\n",
        "    optimizer.zero_grad() # 일단 가장 먼저 해주어야 하는 것은 optimizer에 저장된 gradient 값을 0로 만들어줘야 합니다. 우리는 위에 정의한 SGD에 의해 Loss에 따른 gradient를 계산하게 되고 이를 optimizer에 저장하게 되는데요,\n",
        "    # 이 부분에 대해서는 코드를 끝까지 작성하고 설명할게요.\n",
        "    hypothesis = model(X) # 다음으로는 hypothesis, 즉 y hat 예측값을 구해줍니다.\n",
        "\n",
        "    # cost/loss function\n",
        "    cost = criterion(hypothesis, Y) # 그리고 y hat과 y의 오차를 구해주고요,\n",
        "    cost.backward() # backward 메소드를 통해 cost, 즉 오차를 기반으로 하여 backpopagation을 진행합니다.\n",
        "    optimizer.step() # Backprop 과정이 수식적인 부분이 많고, 이를 수업시간에 다 설명하기 어려워 잠깐 넘어갔는데요, \n",
        "    # 우리는 cost 를 이용해 직접적으로 weight에 영향을 주는 것이 아닌,\n",
        "    # 이 오차를 기반으로 해서 backprop 과정에서 구한 gradient 값으로 weight를 업데이트 해주게 됩니다.\n",
        "    # 그래서 우리는 그 gradient 값을 optimizer에 저장을 해주는데,\n",
        "    # 이렇게 업데이트된 weight에 따른 예측값은 이전과 당연히 다르겠죠?\n",
        "    # 그래서 제일 위에서 zero_grad라는 메소드를 optimizer에 달아준 것입니다.    \n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(step, cost.item()) # cost 출력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "runk6w1sE-CV",
        "outputId": "ae89df23-b554-4e63-c21f-4bf835be5ff0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 0.7121162414550781\n",
            "100 1.0441728830337524\n",
            "200 1.0388953685760498\n",
            "300 0.4784403443336487\n",
            "400 0.4779386520385742\n",
            "500 0.47775983810424805\n",
            "600 0.47766825556755066\n",
            "700 0.47761261463165283\n",
            "800 0.47757524251937866\n",
            "900 0.4775484800338745\n",
            "1000 0.47752830386161804\n",
            "1100 0.4775125980377197\n",
            "1200 0.4775000214576721\n",
            "1300 0.47748976945877075\n",
            "1400 0.4774811565876007\n",
            "1500 0.4774739444255829\n",
            "1600 0.4774676561355591\n",
            "1700 0.47746217250823975\n",
            "1800 0.4774574935436249\n",
            "1900 0.4774532616138458\n",
            "2000 0.47744959592819214\n",
            "2100 0.4774462580680847\n",
            "2200 0.47744327783584595\n",
            "2300 0.47744056582450867\n",
            "2400 0.47743815183639526\n",
            "2500 0.477435827255249\n",
            "2600 0.47743380069732666\n",
            "2700 0.47743189334869385\n",
            "2800 0.4774301052093506\n",
            "2900 0.47742849588394165\n",
            "3000 0.4774269461631775\n",
            "3100 0.47742557525634766\n",
            "3200 0.4774242639541626\n",
            "3300 0.4774230718612671\n",
            "3400 0.4774218797683716\n",
            "3500 0.4774208068847656\n",
            "3600 0.47741979360580444\n",
            "3700 0.47741878032684326\n",
            "3800 0.4774179458618164\n",
            "3900 0.4774170219898224\n",
            "4000 0.47741615772247314\n",
            "4100 0.4774154722690582\n",
            "4200 0.47741472721099854\n",
            "4300 0.47741398215293884\n",
            "4400 0.4774133563041687\n",
            "4500 0.47741273045539856\n",
            "4600 0.4774121046066284\n",
            "4700 0.47741153836250305\n",
            "4800 0.4774109721183777\n",
            "4900 0.4774104952812195\n",
            "5000 0.47740989923477173\n",
            "5100 0.4774094223976135\n",
            "5200 0.4774090051651001\n",
            "5300 0.4774084985256195\n",
            "5400 0.47740811109542847\n",
            "5500 0.47740766406059265\n",
            "5600 0.4774072766304016\n",
            "5700 0.47740688920021057\n",
            "5800 0.47740650177001953\n",
            "5900 0.47740617394447327\n",
            "6000 0.4774058163166046\n",
            "6100 0.47740548849105835\n",
            "6200 0.4774051308631897\n",
            "6300 0.47740480303764343\n",
            "6400 0.47740453481674194\n",
            "6500 0.47740429639816284\n",
            "6600 0.47740399837493896\n",
            "6700 0.4774037003517151\n",
            "6800 0.4774034023284912\n",
            "6900 0.4774031639099121\n",
            "7000 0.47740286588668823\n",
            "7100 0.4774026572704315\n",
            "7200 0.47740235924720764\n",
            "7300 0.4774021506309509\n",
            "7400 0.4774019122123718\n",
            "7500 0.4774017035961151\n",
            "7600 0.4774014949798584\n",
            "7700 0.4774013161659241\n",
            "7800 0.47740110754966736\n",
            "7900 0.47740083932876587\n",
            "8000 0.4774007201194763\n",
            "8100 0.4774005115032196\n",
            "8200 0.47740036249160767\n",
            "8300 0.47740018367767334\n",
            "8400 0.477400004863739\n",
            "8500 0.4773997962474823\n",
            "8600 0.47739964723587036\n",
            "8700 0.47739946842193604\n",
            "8800 0.4773993492126465\n",
            "8900 0.4773991107940674\n",
            "9000 0.4773990511894226\n",
            "9100 0.4773988425731659\n",
            "9200 0.47739869356155396\n",
            "9300 0.477398544549942\n",
            "9400 0.4773983955383301\n",
            "9500 0.4773982763290405\n",
            "9600 0.47739818692207336\n",
            "9700 0.47739797830581665\n",
            "9800 0.4773979187011719\n",
            "9900 0.47739773988723755\n",
            "10000 0.477397620677948\n"
          ]
        }
      ]
    }
  ]
}